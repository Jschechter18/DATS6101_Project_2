---
title: "Modeling"
author: "Josh Schechter"
date: "2025-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(readr)
library(ggplot2)
library(tidyr)
library(caret)
library(pROC)
library(MASS)
```

```{r}
students <- read.csv("../data/Students_Grading_Dataset_Cleaned.csv")
```

```{r}
colnames(students)
```
## Steps:
1. Find features of interest of interest.
2. Create individual regression models for those features.
3. Determine which models are the best.
4. Create a combined regression model, where we basically combine the features from the best models.
5. Use this final regression model to answer smart questions.
6. Report findings.

```{r}
cor(students[, c("Total_Score", "Study_Hours_per_Week", 
                         "Sleep_Hours_per_Night", 
                         "Attendance")], method="pearson")
```
Target variables:
1. Pass_Fail (logistic regression)
2. Total_Score (linear regression?)

```{r}
model <- glm(Pass_Fail ~ Study_Hours_per_Week + 
                   Internet_Access_at_Home + 
                   Parent_Education_Level + 
                   Stress_Level + 
                   Extracurricular_Activities + 
                   Family_Income_Level + 
                   Attendance,
                   data = students, family = binomial)
```


```{r}
# Stepwise selection
stepwise_model <- stepAIC(model, direction = "both")

summary(stepwise_model)
```
Feature selection tells us that attendance is the best model

```{r}
attendance_model <- glm(Pass_Fail ~ Attendance, 
                   data = students, family = binomial)
```


```{r}
summary(attendance_model)
```

```{r}
students$predictions <- predict(attendance_model, type = "response")

students$Pass_Fail <- factor(students$Pass_Fail, levels = c("Fail", "Pass"))
students$predicted_class <- ifelse(students$predictions > 0.6, "Pass", "Fail") # best one thus far
students$predicted_class <- factor(students$predicted_class, levels = c("Fail", "Pass"))
confusionMatrix(students$predicted_class, students$Pass_Fail, positive = "Pass")
```

```{r}
roc(students$Pass_Fail, 
    predict(attendance_model, type = "response"), 
    plot = TRUE, 
    print.auc = TRUE, 
    col = "blue", 
    lwd = 2)
```
*Attendance AUC* -> 0.798



## Can we improve on attendance?

# Feature engineering
```{r}
students$Engagement_Ratio <- (students$Participation_Score + students$Attendance + students$Study_Hours_per_Week) / 3
```

```{r}
# BEST MODEL
engagement_model <- glm(Pass_Fail ~ Engagement_Ratio,
                   data = students, family = binomial)
```


```{r}
summary(engagement_model) # engagement is most significant right now
```


```{r}
students$predictions <- predict(engagement_model, type = "response")

students$Pass_Fail <- factor(students$Pass_Fail, levels = c("Fail", "Pass"))
students$predicted_class <- ifelse(students$predictions > 0.6, "Pass", "Fail") # best one thus far
students$predicted_class <- factor(students$predicted_class, levels = c("Fail", "Pass"))
confusionMatrix(students$predicted_class, students$Pass_Fail, positive = "Pass")
```

```{R}
roc(students$Pass_Fail,
    students$predictions,
    plot = TRUE, 
    print.auc = TRUE, 
    col = "blue", 
    lwd = 2)
```

Auc went down upon feature engineering. However, the recall went up

