---
title: "Indicators of student success in a college level course"
author: " Josh Schechter, Naiyani Paladugu, Lincoln Orellana"
date: "05-04-2025"
output:
  html_document:
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction -> Josh
## Data Preprocessing -> Lincoln
## Exploratory Data Analysis (EDA) -> Lincoln
## Linear Regression -> Josh
## Logistic regression -> Josh
## Random Forest Model -> Naiyani
## Comparing Random Forest and logistic Regression Models -> Naiyani
## Conclusion -> Naiyani

```{r}
library(dplyr)
library(readr)
library(ggplot2)
library(tidyr)
library(caret)
library(pROC)
library(MASS)
library(rpart)
library(randomForest)
```

# Introduction
When constructing and teaching a college level course over a long period of time, a professor should generally be able to identify tendencies and characteristics of students who are likely to perform well in the course. If the professor can identify these tendencies, it can help to ensure that the course is being taught in a way that is accessible and fair to all students, provide proper guidance for students to know what will be likely to make them successful, and identify what improvements could be made in the course based on previous data. It is our goal to identify the characteristics, tendencies, and predictors of students who are unlikely to success in the course.

Our research questions are as follows:
1. What features are the best predictors of student success in a college level course?
2. Can we create a model that accurately predicts passing/failing?



```{r}
students <- read.csv("../data/Students_Grading_Dataset_Cleaned.csv")
```

# Data Preprocessing ***LINCOLN***

# Exploratory Data Analysis (EDA) ***LINCOLN***
```{r}
colnames(students)
```

```{r}
cor_mat <- cor(students[, c("Total_Score", "Study_Hours_per_Week", 
                         "Sleep_Hours_per_Night", 
                         "Attendance", "Participation_Score",
                         'Pass_Fail')])


corrplot::corrplot(cor_mat,
                   method="number",
                   col="blue",
                   type="lower",
                   cl.pos='n',
                   diag=FALSE)
```


```{r}
str(students)
summary(students)
```

```{r}
table(students$Pass_Fail)
barplot(table(students$Pass_Fail), col=c('orange','blue'), main = "Distribution of Pass/Fail")
```


```{r}
ggplot(students, aes(x = Total_Score)) + 
  geom_histogram(binwidth = 2, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Total Score")

ggplot(students, aes(x = Study_Hours_per_Week)) + 
  geom_histogram(fill = "purple", color = "white") +
  labs(title = "Study Hours per Week Distribution")

ggplot(students, aes(x = Attendance)) + 
  geom_histogram(fill = "green", color = "white") +
  labs(title = "Attendance Distribution")
```

```{r}
students %>%
  count(Family_Income_Level, Pass_Fail) %>%
  group_by(Family_Income_Level) %>%
  mutate(Proportion = n / sum(n)) %>%
  ggplot(aes(x = Family_Income_Level, y = Proportion, fill = Pass_Fail)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Normalized Pass_Fail Distribution by Family Income Level",
       x = "Family Income Level", y = "Proportion") +
  ylim(0,0.5)
```


# Finalize data cleaning before modeling:
```{r}
students$Total_Score <- as.numeric(students$Total_Score)
students$Study_Hours_per_Week <- as.numeric(students$Study_Hours_per_Week)
students$Sleep_Hours_per_Night <- as.numeric(students$Sleep_Hours_per_Night)
students$Stress_Level <- as.numeric(students$Stress_Level)
```

## Target variables:
1. Pass_Fail (logistic regression)
2. Total_Score (linear regression?

# Upsampling our imbalanced dataset
```{r}
students$Pass_Fail <- factor(students$Pass_Fail, levels = c(0, 1), labels = c("Fail", "Pass"))
set.seed(123)  # for reproducibility
students <- upSample(x = students[, setdiff(names(students), "Pass_Fail")],
                               y = students$Pass_Fail,
                               yname = "Pass_Fail")

# Check class balance
table(students$Pass_Fail)
```
```{r}
students$Pass_Fail <- ifelse(students$Pass_Fail == "Fail", 0, 1)
```


# Linear Regression ***JOSH***

After our findings in EDA, we wanted to see if we could create a linear model for infering total score based on predictors. Our initial thought was to use the following predictors:
1. Attendance
2. Sleep Hours per Night
3. Study Hours per Week
4. Stress Level
5. Family Income Level
6. Extracurricular Activities
7. Internet Access at Home
From there, we would run a stepwise regression to see if we could find a model with less predictors that still yields similar results.


# Linear regressions total grade
```{r}
full_lin_reg_model <- lm(Total_Score ~ Attendance + Sleep_Hours_per_Night + Study_Hours_per_Week + Stress_Level + Family_Income_Level + Extracurricular_Activities + Internet_Access_at_Home, data = students)
summary(full_lin_reg_model)

lin_reg_stepwise_model <- stepAIC(full_lin_reg_model, direction = "both", trace = FALSE)

# View selected model summary
summary(lin_reg_stepwise_model)
```
Post stepwise regression, we were left with four predictors:
1. Study Hours per Week
2. Stress Level
3. Family Income Level-Low
4. Family Income Level-Medium

With an alpha= 0.05, only family income level-low had a statistically significant p-value. This is an indication that family income level does in fact have some sort of a significant relationship with inferring total score in this linear regression model. The other predictors were not statistically significant, however they were recommended by the stepwise algorithm, which indicates that they did impact the model.

Our linear regression formula is as follows:
y = 74.38158 + -0.04728 * Study_Hours_per_Week + 0.11350 * Stress_Level + 1.59011 * Family_Income_LevelLow + 0.68504 * Family_Income_LevelMedium

Notably, our model is statistically very weak, with a multiple r-squared value of 0.00284, indicating that our model only explains 0.284% of the variance in total score. Given this is a very low percentage, we can conclude that this model alone is not a good predictor of total score.


# Logistic Regression ***JOSH***

The next model we set out to build was a logistic regression model to predict pass/fail based on the same indicators as our linear regression model. Once again, we ran a stepwise model comparison to determine which model would yield the best results with the least amount of predictors.

## Logistic regression
```{r}
full_logit_model <- glm(Pass_Fail ~ Attendance + Family_Income_Level + Extracurricular_Activities + Internet_Access_at_Home + Sleep_Hours_per_Night + Study_Hours_per_Week + Stress_Level,
                   data = students,
                   family = binomial)

# Stepwise selection
stepwise_logit_model <- stepAIC(full_logit_model, direction = "both")

summary(stepwise_logit_model)
```
Post stepwise regression, we were left with just tree predictors:
1. Attendance
2. Family Income Level-Low
3. Family Income Level-Medium

With an alpha=0.05-of the selected predictors-only attendance was statistically significant given its p-value of 0.00. Notably, family income level-low was very close to the alpha value, although just missed the threshold. This would indicate that family income level-low does have some sort of a relevant relationship with pass/fail. Family income level-median was not remotely close to statistical significance. At this point, we will make our final logistic regression model.S


```{r}
final_logit_model <- glm(Pass_Fail ~ Attendance + Family_Income_Level, 
                   data = students)

summary(final_logit_model)
```


```{r}
students$predictions <- predict(final_logit_model, type = "response")
```

Now that our model is built, and we have made our probabilities for each datapoint via the model, we will set the threshold to 0.5, as the dataset has been upsampled already.
```{r}
students$predicted_class <- ifelse(students$predictions > 0.5, 1, 0)
students$predicted_class <- factor(students$predicted_class, levels = c(0, 1), labels = c("Fail", "Pass"))
```


```{r}
students$Pass_Fail <- as.numeric(as.character(students$Pass_Fail))
students$Pass_Fail <- factor(students$Pass_Fail, levels = c(0, 1), labels = c("Fail", "Pass"))
```

```{r}
table(students$predicted_class, useNA = "always")
table(students$Pass_Fail, useNA = "always")
```


```{r}
# Now evaluate with confusion matrix
students$Pass_Fail <- factor(students$Pass_Fail, levels = c("Fail", "Pass"))
confusionMatrix(students$predicted_class, students$Pass_Fail, positive = "Pass")
```

To analyze the results, we built a confusion matrix. The main metrics that we will be looking at are:
1. Sensitivity (Recall for pass), for all actual passing students, what proportion was predicted as passing?
2. Specificity (Recall for fail), for all actual failing students, what proportion was predicted as failing?

For sensitivity, the score was 0.6935. It can be said that 69.35% of all actual passing students were predicted to pass the course. For specificity, the score was 0.7144. It can be said that 71.44% of all actual failing students were predicted to fail the course. For our use case, we can make the claim that this is a good model. As not only is it approximately good at predicting both, but it is able to identify students who are likely to fail the course at a fairly high rate.


```{r}
conf_mat <- confusionMatrix(students$predicted_class, students$Pass_Fail, positive = "Pass")
conf_mat_df <- as.data.frame(conf_mat$table)
ggplot(data = conf_mat_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "black") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  scale_y_discrete(limits = rev(levels(conf_mat_df$Prediction))) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix",
    x = "Actual (Reference)",
    y = "Predicted"
  )
```

```{r}
table(students$predicted_class, useNA = "always")
table(students$Pass_Fail, useNA = "always")
```


```{r}
roc(students$Pass_Fail,
    main="ROC Curve for Attendance Model",
    predict(final_logit_model, type = "response"), 
    plot = TRUE, 
    print.auc = TRUE, 
    col = "blue", 
    lwd = 2)
```

Finally, we can analyze the ROC curve to visually see the tradeoff between Sensitivity and Specificity. The AUC represents the area under the curve, where a higher number will indicate that the model is better than random guessing. An AUC score of 0.8 is acceptable. In our model, the area under the curve (AUC) is 0.804, which indicates that the model is able to distinguish between passing and failing students with a good level of accuracy. Therefore, it can be said that this logistic regression model is fairly good for predicting pass/fail.


## Random Forest Modeling
# Growing tree
```{r}
# Building random forest model
set.seed(1)
rf_model <- randomForest(Pass_Fail ~ Attendance + Family_Income_Level + Extracurricular_Activities + Internet_Access_at_Home,
                         data = students,
                         ntree = 500,
                         mtry = 2,
                         importance = TRUE)

print(rf_model)
```

We chose to use a Random Forest model — it’s a powerful machine learning technique that works by combining several decision trees to make better predictions.
We selected the following features based on their potential influence on academic performance:
- Attendance – a key factor in student engagement and learning.
- Family Income Level – which can affect access to resources and overall academic support.
- Extracurricular Activities – which may impact time management and stress.
- Internet Access at Home – especially important in a digital learning environment.
These variables were used to train our model and understand patterns in student success and risk.


```{r}
rf_predictions <- predict(rf_model, students, type = "class")

confusionMatrix(rf_predictions, students$Pass_Fail, positive = 'Pass')
```

**Analysing Results:**
Overall Accuracy : 0.7588 - We can see the model achieved an overall accuracy of 75.88%, meaning it correctly classified about three out of every four students.
95% CI : 74.58% to 77.16% , giving us a solid statistical range.
p value: 2.2e-16 (confirming that the model's accuracy is statistically significant)
Pos Pred Value: 0.9712
Neg Pred Value: 0.6784
Prevalence: 0.5000
Detection Rate: 0.2667
Detection Prevalence: 0.2744

Sensitivity (Recall for pass): 0.5335 - moderate range
 -True positive rate for students who passed, is 0.5335, meaning the model correctly identifies about 53% of students who pass.
Specificity (Recall for fail): 0.9842 - high range
 - This means that the model is able to correctly identify 98.42% of the students who failed.


```{r}
tree_conf_mat <- confusionMatrix(rf_predictions, students$Pass_Fail, positive = 'Pass')
tree_conf_mat_df <- as.data.frame(tree_conf_mat$table)
ggplot(data = tree_conf_mat_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "black") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  scale_y_discrete(limits = rev(levels(conf_mat_df$Prediction))) +
  theme_minimal() +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted")
```

**Confusion Matrix**:
On the x-axis, we have the actual outcomes — whether students actually passed or failed.
On the y-axis, we have the model’s predictions.
- The model correctly predicted 2,116 students would fail, and they did — this is the true negatives.
- It also correctly predicted 1,147 students would pass — the true positives.
- However, it incorrectly predicted that 1,003 students would fail, when they actually passed — these are false negatives.
- And only 34 students were wrongly predicted to pass when they actually failed — the false positives.

What’s striking here is that the model is very cautious — it's far more likely to predict failure.
-This leads to many false negatives, meaning a significant number of students who would pass are being predicted as likely to fail.
-This supports what we saw earlier — the model has high specificity but moderate sensitivity

```{r}
rf_probs <- predict(rf_model, students, type = "prob")[,2]

rf_roc <- roc(students$Pass_Fail, rf_probs)
plot(rf_roc,
     main="ROC Curve - Random Forest",
     print.auc = TRUE, 
     col = "blue", 
     lwd = 2)
auc(rf_roc)
```

**ROC Curve**:
- The ROC curve helps us visualize the trade-off between sensitivity and specificity at various threshold levels.
- As a rule of thumb, an AUC — or Area Under the Curve — greater than 0.8 is considered good, and anything above 0.85 is excellent.

In our case, the AUC is 0.896, which is excellent!
This tells us that the model is highly capable of distinguishing between students who will pass and those who will fail — even when we vary the decision threshold.
So, overall, this ROC analysis reinforces that our model performs very well.

# Variable Importance plot
```{r}

importance(rf_model)

varImpPlot(rf_model, 
           main = "Variable Importance - Random Forest",
           col = "blue", 
           lwd = 2)
```

**Variable importance plot** 
- Helps us to confirm that attendance is the most important variable in predicting pass/fail
- Supporting our initial assumption (the results of the logistic regression model)
- This plot visualizes how much each variable contributes to the predictive accuracy of the Random Forest model
- The higher the mean decrease in accuracy or Gini index, the more important the variable
- In this case, the large contribution of attendance indicates that it plays a central role in student performance


## Comparing Logistic Regression to Random Forest:
**Logistic Regression**
 - Detects pass students 90.56% of the time
 - Detects fail students 44.00% of the time
 - AUC(ROC): 0.804

**Random Forest**
 - Detects pass students 53.35% of the time
 - Detects fail students 98.42% of the time
 - AUC(ROC): 0.896

**Comparing the performance of Logistic Regression and Random Forest** based on three key metrics: Pass Recall, Fail Recall, and AUC.
- Pass Recall is much higher for Logistic Regression — about 90.56%, compared to 53.35% for Random Forest. So Logistic Regression is better at identifying students who will pass.
- However, Fail Recall tells a different story — Random Forest outperforms Logistic Regression significantly, correctly identifying 98.42% of students who will fail, compared to only 44% by Logistic Regression.
- Looking at the AUC, Random Forest scores 0.896, which is excellent, while Logistic Regression has 0.804, which is still good but not as strong.

So, if the professor's goal is to identify students who are at risk of failing, then Random Forest is the better choice.But if the focus is on recognizing students likely to succeed, Logistic Regression would be more appropriate.
Ultimately, the choice of model depends on the specific goals of the professor or the intervention strategy being used

**Analysis**
 - The random forest model is excellent at detecting students likely to fail, but poor at detecting passing students.
 - The logistic regression model is excellent at detecting students likely to pass, but poor at detecting failing students.
 
 - Which model to use is dependent on which the professor is more interested in catching. If the professor wants to predict which students are likely to fail, use the random forest. If the professor wants to predict which students will pass, use the logistic regression model.






